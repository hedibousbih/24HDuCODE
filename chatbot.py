import os
import uuid
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import StateGraph, START, MessagesState
from langgraph.checkpoint.memory import MemorySaver
from dotenv import load_dotenv, find_dotenv
from agent_config import agent

load_dotenv(find_dotenv())
print("Bienvenue Ã  lâ€™HÃ´tel California ! Camille est lÃ  pour vous aider.")
print("Tapez 'exit' pour quitter.")

while True:
    user_input = input("Vous > ").strip()
    if user_input.lower() in ["exit", "quit"]:
        print("Camille > Merci pour votre visite. Ã€ bientÃ´t ! ğŸŒŸ")
        break

    response = agent.invoke(user_input)
    print("Camille >", response)

api_key = os.getenv("MISTRAL_API_KEY")
if not api_key:
    raise ValueError("MISTRAL_API_KEY not found in .env file")

# Charger la clÃ© API Mistral
if not os.environ.get("MISTRAL_API_KEY"):
    from getpass import getpass
    os.environ["MISTRAL_API_KEY"] = getpass("Enter Mistral API key: ")

# Initialiser le modÃ¨le Mistral
model = init_chat_model("mistral-large-latest", model_provider="mistralai")

# CrÃ©er le prompt de Camille
prompt_template = ChatPromptTemplate.from_messages([
    SystemMessage(content=(
        "Tu es Camille, l'assistante virtuelle de lâ€™HÃ´tel California. "
        "Tu es serviable, polie, toujours prÃªte Ã  aider les clients. "
        "Tu parles plusieurs langues, principalement le franÃ§ais et lâ€™anglais. "
        "Adapte ta rÃ©ponse Ã  la langue du client automatiquement. "
        "Si tu ne comprends pas, demande poliment plus de dÃ©tails. "
        "Fais tout ton possible pour crÃ©er une expÃ©rience agrÃ©able."
    )),
    MessagesPlaceholder(variable_name="messages")
])

# Fonction de traitement des messages
def call_model(state: MessagesState):
    prompt = prompt_template.invoke(state)
    response = model.invoke(prompt)
    return {"messages": state["messages"] + [response]}

# CrÃ©er le graph LangGraph
workflow = StateGraph(state_schema=MessagesState)
workflow.add_node("model", call_model)
workflow.add_edge(START, "model")

# MÃ©moire persistante
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# Initialiser la session utilisateur
user_id = input("Entrez votre prÃ©nom : ").strip().lower()
config = {"configurable": {"thread_id": f"user_{user_id}"}}
messages = []

print("\nBienvenue Ã  l'HÃ´tel California ! Camille est lÃ  pour vous aider ğŸ˜Š")
print("Tapez 'exit' pour quitter.\n")

# Boucle de conversation
while True:
    user_input = input(f"{user_id} > ").strip()
    if user_input.lower() in {"exit", "quit"}:
        print("Camille > Merci de votre visite. Ã€ bientÃ´t ! ğŸŒŸ")
        break

    messages.append(HumanMessage(content=user_input))
    result = app.invoke({"messages": messages}, config)
    response = result["messages"][-1]
    messages = result["messages"]

    print(f"Camille > {response.content}")
