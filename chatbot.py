import os
import uuid
from langchain.chat_models import init_chat_model
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import StateGraph, START, MessagesState
from langgraph.checkpoint.memory import MemorySaver
from dotenv import load_dotenv, find_dotenv
from agent_config import agent

load_dotenv(find_dotenv())
print("Bienvenue Ã  lâ€™HÃ´tel California ! Camille est lÃ  pour vous aider.")
print("Tapez 'exit' pour quitter.")

while True:
    user_input = input("Vous > ").strip()
    if user_input.lower() in ["exit", "quit"]:
        print("Camille > Merci pour votre visite. Ã€ bientÃ´t ! ğŸŒŸ")
        break

    response = agent.invoke(user_input)
    print("Camille >", response)

api_key = os.getenv("MISTRAL_API_KEY")
if not api_key:
    raise ValueError("MISTRAL_API_KEY not found in .env file")

# Charger la clÃ© API Mistral
if not os.environ.get("MISTRAL_API_KEY"):
    from getpass import getpass
    os.environ["MISTRAL_API_KEY"] = getpass("Enter Mistral API key: ")

# Initialiser le modÃ¨le Mistral
model = init_chat_model("mistral-large-latest", model_provider="mistralai")

# CrÃ©er le prompt de Camille
prompt_template = ChatPromptTemplate.from_messages([
    SystemMessage(content=(
        "Tu es Camille, l'assistante virtuelle de lâ€™HÃ´tel California. "
        "Tu es serviable, polie, toujours prÃªte Ã  aider les clients. "
        "Tu parles plusieurs langues, principalement le franÃ§ais et lâ€™anglais. "
        "Adapte ta rÃ©ponse Ã  la langue du client automatiquement. "
        "Si tu ne comprends pas, demande poliment plus de dÃ©tails. "
        "Fais tout ton possible pour crÃ©er une expÃ©rience agrÃ©able."
        "Ne rÃ©ponds pas directement Ã  l'utilisateur et n'indique pas la rÃ©ponse finale tant que tu n'as pas terminÃ© tous les appels d'outils nÃ©cessaires."
        "Si l'utilisateur parle dâ€™un moment de la journÃ©e (ex: ce soir, ce midi), associe-le automatiquement Ã  un repas disponible."
        "Ne donne jamais la rÃ©ponse finale tant que tous les outils nÃ©cessaires n'ont pas Ã©tÃ© appelÃ©s et que toutes les observations ont Ã©tÃ© reÃ§ues."
        "Quand lâ€™utilisateur mentionne un repas comme 'dÃ©jeuner', 'dÃ®ner', ou 'souper', "
        "traduis-le automatiquement en nom exact de repas disponible via lâ€™API, comme 'Lunch' ou 'Dinner'. "
        "Nâ€™utilise que les noms de repas attendus par lâ€™API ('Breakfast', 'Lunch', 'Dinner')."
        "Important : Dans chaque rÃ©ponse, ne produis soit une action (avec Action: ...), soit une rÃ©ponse finale (avec Final Answer: ...), mais jamais les deux en mÃªme temps. Sinon, le systÃ¨me Ã©chouera."


    )),
    MessagesPlaceholder(variable_name="messages")
])

# Fonction de traitement des messages
def call_model(state: MessagesState):
    prompt = prompt_template.invoke(state)
    response = model.invoke(prompt)
    return {"messages": state["messages"] + [response]}

# CrÃ©er le graph LangGraph
workflow = StateGraph(state_schema=MessagesState)
workflow.add_node("model", call_model)
workflow.add_edge(START, "model")

# MÃ©moire persistante
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# Initialiser la session utilisateur
user_id = input("Entrez votre prÃ©nom : ").strip().lower()
config = {"configurable": {"thread_id": f"user_{user_id}"}}
messages = []

print("\nBienvenue Ã  l'HÃ´tel California ! Camille est lÃ  pour vous aider ğŸ˜Š")
print("Tapez 'exit' pour quitter.\n")

# Boucle de conversation
while True:
    user_input = input(f"{user_id} > ").strip()
    if user_input.lower() in {"exit", "quit"}:
        print("Camille > Merci de votre visite. Ã€ bientÃ´t ! ğŸŒŸ")
        break

    messages.append(HumanMessage(content=user_input))
    result = app.invoke({"messages": messages}, config)
    response = result["messages"][-1]
    messages = result["messages"]

    print(f"Camille > {response.content}")
